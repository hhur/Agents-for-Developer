{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of using hugging face model hub with OpenVINO\n",
    "\n",
    "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/hugging-face-hub/hugging-face-hub.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu \"transformers>=4.33.0\" \"torch>=2.1.0\"\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a Model Using the HF Transformers Package\n",
    "\n",
    "Twitter-roBERTa-base for Sentiment Analysis   is used.  This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark.  \n",
    "Use AutoModelForSequenceClassification to initialize the model and perform inference with it\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, return_dict=True)\n",
    "\n",
    "# The torchscript=True flag is used to ensure the model outputs are tuples\n",
    "# instead of ModelOutput (which causes JIT errors).\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, torchscript=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the classifcation of a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.8178\n",
      "2) neutral 0.1685\n",
      "3) negative 0.0137\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "    Only the brave men and women can bring peace to the world, not by practicing war but by practicing nonviolence. \n",
    "    Our heart is wide enough to embrace the world and hands are long enough to encompass the world.\n",
    "    \"\"\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "scores = output[0][0]\n",
    "scores = torch.softmax(scores, dim=0).numpy(force=True)\n",
    "\n",
    "def print_prediction(scores):\n",
    "    for i, descending_index in enumerate(scores.argsort()[::-1]):\n",
    "        label = model.config.id2label[descending_index]\n",
    "        score = np.round(float(scores[descending_index]), 4)\n",
    "        print(f\"{i+1}) {label} {score}\")\n",
    "\n",
    "print_prediction(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the Model to OpenVINO IR format\n",
    "\n",
    "use the OpenVINO Model conversion API to convert the model (this one is implemented in PyTorch) to OpenVINO Intermediate Representation (IR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "save_model_path = Path('./models/model.xml')\n",
    "\n",
    "if not save_model_path.exists():\n",
    "    ov_model = ov.convert_model(model, example_input=dict(encoded_input))\n",
    "    ov.save_model(ov_model, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run model inference on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPU', 'GPU', 'NPU']\n",
      "1) positive 0.8165\n",
      "2) neutral 0.1698\n",
      "3) negative 0.0136\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "devices = core.available_devices\n",
    "print(devices)\n",
    "\n",
    "device = 'GPU'\n",
    "compiled_model = ov.Core().compile_model(save_model_path, device)\n",
    "\n",
    "# Compiled model call is performed using the same parameters as for the original model\n",
    "scores_ov = compiled_model(encoded_input.data)[0]\n",
    "\n",
    "scores_ov = torch.softmax(torch.tensor(scores_ov[0]), dim=0).detach().numpy()\n",
    "\n",
    "print_prediction(scores_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "\n",
    "model = OVModelForSequenceClassification.from_pretrained(MODEL, export=True, device='GPU')\n",
    "\n",
    "# The save_pretrained() method saves the model weights to avoid conversion on the next load.\n",
    "model.save_pretrained('./models/optimum_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export openvino --help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export openvino --model $MODEL --task text-classification --fp16 models/optimum_model/fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to GPU ...\n",
      "Setting OpenVINO CACHE_DIR to models\\optimum_model\\fp16\\model_cache\n"
     ]
    }
   ],
   "source": [
    "model = OVModelForSequenceClassification.from_pretrained(\"models/optimum_model/fp16\", device='GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.8165\n",
      "2) neutral 0.1698\n",
      "3) negative 0.0136\n"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "scores = output[0][0]\n",
    "scores = torch.softmax(scores, dim=0).numpy(force=True)\n",
    "\n",
    "print_prediction(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
