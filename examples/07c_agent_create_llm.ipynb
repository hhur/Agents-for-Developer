{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "# Get the prompt to use \n",
    "# https://github.com/hwchase17/langchain-hub?tab=readme-ov-file\n",
    "#prompt = load_prompt('lc://prompts/conversation/prompt.json')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\",optional=True\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"agent_scratchpad\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "prompt.messages\n",
    "#prompt.pretty_print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] partial_variables={'instructions': 'You are an assistant.\\nPlease answer in a structured, clear, coherent, and comprehensive manner.\\nIf the content has sections, please summarize them in order and present them easy to read format.\\nif you are using table or figure number then make sure to add reference at the footnote.\\nplease mention reference sources.\\nPlease make sure to provide the answer in a structured, clear, coherent, and comprehensive manner.\\n'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "instructions = \"\"\"You are an assistant.\n",
    "Please answer in a structured, clear, coherent, and comprehensive manner.\n",
    "If the content has sections, please summarize them in order and present them easy to read format.\n",
    "if you are using table or figure number then make sure to add reference at the footnote.\n",
    "please mention reference sources.\n",
    "Please make sure to provide the answer in a structured, clear, coherent, and comprehensive manner.\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = hub.pull(\"hwchase17/react\")\n",
    "react_prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "\n",
    "print(react_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent, Tool\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def build(commit_id: int):\n",
    "    \"\"\"build artifact with commit_id .\"\"\"\n",
    "    print(f\"Building artifact with commit_id {commit_id}\")\n",
    "    build_id = 100\n",
    "    return build_id\n",
    "\n",
    "@tool\n",
    "def deploy(build_id: int):\n",
    "    \"\"\"deploy artifact with build_id. \"\"\"\n",
    "    print(f\"Deploying artifact with build_id {build_id}\")\n",
    "    deploy_id = 200\n",
    "    return deploy_id\n",
    "\n",
    "@tool\n",
    "def test(deploy_id: int):\n",
    "    \"test artifact with deploy_id.\"\n",
    "    print(f\"Testing artifact with deploy_id {deploy_id}\")\n",
    "    test_id = 300\n",
    "    return test_id\n",
    "\n",
    "@tool\n",
    "def report(test_id: int):\n",
    "    \"generate report with test_id.\"\n",
    "    print(f\"Generating report with test_id {test_id}\")\n",
    "    report_id = 400\n",
    "    return report_id\n",
    "\n",
    "#build.invoke({\"commit_id\": 1})\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Union\n",
    "\n",
    "from langchain.output_parsers import JsonOutputToolsParser\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"gpt-4-0125-preview\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "tools = [build, deploy, test, report, \n",
    "         Tool(\n",
    "             name=\"reason\",\n",
    "             func=llm_chain.invoke,\n",
    "             description = \"assistant to help you with the reason\",\n",
    "         ),]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCXL can refer to different things depending on the context. Here are a few possibilities:\n",
      "\n",
      "1. **Compute Express Link (CXL):** This is perhaps the most prominent reference in the field of computing and technology. Compute Express Link (CXL) is an open industry standard interconnect offering high-speed, efficient performance for data centers and computing environments. It's designed to improve the performance of data centers by enabling high-speed, efficient communication between the CPU (Central Processing Unit) and workload accelerators, memory buffers, and smart I/O devices. CXL is significant for its support of heterogenous computing and its ability to maintain memory coherency among devices. This makes it particularly useful for applications in AI, machine learning, and high-performance computing.\n",
      "\n",
      "2. **Customer Experience Lifecycle (CXL):** In marketing and customer service, CXL often refers to the Customer Experience Lifecycle. This concept involves understanding and managing the entire journey a customer has with a company or brand, from awareness through consideration, purchase, and loyalty. The goal is to optimize interactions at every point in this lifecycle to improve customer satisfaction, loyalty, and value.\n",
      "\n",
      "3. **CXL (Roman Numerals):** In Roman numerals, CXL represents the number 140.\n",
      "\n",
      "4. **Other Specific Applications:** In other contexts, CXL might refer to specific products, companies, or services that use the acronym for their branding or naming.\n",
      "\n",
      "Without more context, it's difficult to pinpoint which specific \"CXL\" you're referring to. If you have a specific area or industry in mind, please provide more details for a more accurate explanation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is CXL',\n",
       " 'output': 'CXL can refer to different things depending on the context. Here are a few possibilities:\\n\\n1. **Compute Express Link (CXL):** This is perhaps the most prominent reference in the field of computing and technology. Compute Express Link (CXL) is an open industry standard interconnect offering high-speed, efficient performance for data centers and computing environments. It\\'s designed to improve the performance of data centers by enabling high-speed, efficient communication between the CPU (Central Processing Unit) and workload accelerators, memory buffers, and smart I/O devices. CXL is significant for its support of heterogenous computing and its ability to maintain memory coherency among devices. This makes it particularly useful for applications in AI, machine learning, and high-performance computing.\\n\\n2. **Customer Experience Lifecycle (CXL):** In marketing and customer service, CXL often refers to the Customer Experience Lifecycle. This concept involves understanding and managing the entire journey a customer has with a company or brand, from awareness through consideration, purchase, and loyalty. The goal is to optimize interactions at every point in this lifecycle to improve customer satisfaction, loyalty, and value.\\n\\n3. **CXL (Roman Numerals):** In Roman numerals, CXL represents the number 140.\\n\\n4. **Other Specific Applications:** In other contexts, CXL might refer to specific products, companies, or services that use the acronym for their branding or naming.\\n\\nWithout more context, it\\'s difficult to pinpoint which specific \"CXL\" you\\'re referring to. If you have a specific area or industry in mind, please provide more details for a more accurate explanation.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"build 256 and generate report\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
