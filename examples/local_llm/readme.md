# Local LLM with OpenVINO

OpenVINO™ is an open-source toolkit for optimizing and deploying AI inference. 
OpenVINO™ Runtime can enable running the same model optimized across various hardware devices. 
It can be used to run inference on CPU, GPU and NPU


references

* https://python.langchain.com/docs/integrations/llms/openvino
* https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide/llm-inference-hf.html
* https://docs.openvino.ai/2024/notebooks/254-rag-chatbot-with-output.html
* https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/254-llm-chatbot/254-rag-chatbot.ipynb



