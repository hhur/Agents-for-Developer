{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenVINO Local Pipelines with HuggingFace\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade-strategy eager \"optimum[openvino,nncf]\" --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorization token already provided\n"
     ]
    }
   ],
   "source": [
    "## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "try:\n",
    "    whoami()\n",
    "    print('Authorization token already provided')\n",
    "except OSError:\n",
    "    notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98ab33bfe2b49b9b393fa76bc88782b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "The model weights will be quantized to int8.\n",
      "Using framework PyTorch: 2.2.2+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "c:\\workspace\\llm\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "c:\\workspace\\llm\\.venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "c:\\workspace\\llm\\.venv\\lib\\site-packages\\optimum\\exporters\\onnx\\model_patcher.py:301: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "c:\\workspace\\llm\\.venv\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:120: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "c:\\workspace\\llm\\.venv\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:676: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [Errno 2] No such file or directory>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "|   Num bits (N) | % all parameters (layers)   | % ratio-defining parameters (layers)   |\n",
      "+================+=============================+========================================+\n",
      "|              8 | 100% (226 / 226)            | 100% (226 / 226)                       |\n",
      "+----------------+-----------------------------+----------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d0a6fac71c4edf886e501cb372c477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to GPU ...\n",
      "Exception ignored in: <finalize object at 0x17a7df2ba00; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"C:\\Python39\\lib\\tempfile.py\", line 820, in _cleanup\n",
      "    cls._rmtree(name)\n",
      "  File \"C:\\Python39\\lib\\tempfile.py\", line 816, in _rmtree\n",
      "    _shutil.rmtree(name, onerror=onerror)\n",
      "  File \"C:\\Python39\\lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"C:\\Python39\\lib\\shutil.py\", line 629, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"C:\\Python39\\lib\\tempfile.py\", line 808, in onerror\n",
      "    cls._rmtree(path)\n",
      "  File \"C:\\Python39\\lib\\tempfile.py\", line 816, in _rmtree\n",
      "    _shutil.rmtree(name, onerror=onerror)\n",
      "  File \"C:\\Python39\\lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"C:\\Python39\\lib\\shutil.py\", line 610, in _rmtree_unsafe\n",
      "    onerror(os.scandir, path, sys.exc_info())\n",
      "  File \"C:\\Python39\\lib\\shutil.py\", line 607, in _rmtree_unsafe\n",
      "    with os.scandir(path) as scandir_it:\n",
      "NotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\test1\\\\AppData\\\\Local\\\\Temp\\\\tmpo5gkdyrp\\\\openvino_model.bin'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\"device\": \"GPU\", \"ov_config\": ov_config},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 1000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: i can fly and i can swim, what am i?\\n'\n",
      " '\\n'\n",
      " \"Answer: Let's think step by step. A creature that can fly is typically an \"\n",
      " 'animal with wings, such as a bird or a bat. A creature that can swim is '\n",
      " 'typically an animal that lives in water, such as a fish or a frog. However, '\n",
      " 'birds and bats cannot swim, and fish and frogs cannot fly. Therefore, the '\n",
      " 'answer is not a bird, bat, fish, or frog.\\n'\n",
      " '\\n'\n",
      " 'The only creature that can both fly and swim is a flying fish. Flying fish '\n",
      " 'are not typical fish, but rather a type of fish that can jump out of the '\n",
      " 'water and glide through the air for short distances using their large, '\n",
      " 'wing-like pectoral fins.\\n'\n",
      " '\\n'\n",
      " 'Therefore, the answer is a flying fish.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | ov_llm\n",
    "\n",
    "question = \"i can fly and i can swim, what am i?\"\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(chain.invoke({\"question\": question}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
